{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6toho7uRO7v"
      },
      "source": [
        "# Question 1\n",
        "## Developing an Artificial Neural Network from Scratch.\n",
        "\n",
        "In this notebook, we will be developing a fully connected feedforward neural network.\n",
        "\n",
        "We will import the MNIST dataset from keras datsets. The MNIST dataset contains images of 28x28 pixels each having values ranging from 0-255.\n",
        "It has 60000 images in the training set and 10000 images in the test set. However, we will only use the first 10000 images for training and first 1000 images for testing because our code isn't optimized and it takes time to run. We are not looking for accuracy of our network right now, we will be doing that in the next question when we will be implementing the same using Tensorflow.\n",
        "\n",
        "\n",
        "Run the first 3 cells. Your code begins after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI17X78rktdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrINntzulT4M",
        "outputId": "72879853-cec4-4d1e-dd27-245e969cd0b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr4rLzb9ZBQE"
      },
      "source": [
        "As discussed in the class, the images are flattened to a column.\n",
        "\n",
        "Then we are normalizing them by dividing by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jy7CLWCEwfn"
      },
      "outputs": [],
      "source": [
        "train_X=train_X.reshape(60000,784,1)    # flattening\n",
        "test_X=test_X.reshape(10000,784,1)\n",
        "\n",
        "train_y=train_y.reshape(60000,1)\n",
        "test_y=test_y.reshape(10000,1)\n",
        "\n",
        "train_X= train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X=train_X[:10000]         #taking the first 10000 images.\n",
        "train_y=train_y[:10000]\n",
        "test_X=test_X[:1000]\n",
        "test_y=test_y[:1000]\n",
        "train_data=list(zip(train_X,train_y))\n",
        "test_data=list(zip(test_X,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWwDzh6kZOy3"
      },
      "source": [
        "## 1.1 Write the code for Sigmoid Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q5a8tGYku-7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return \"\"\"....\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJI5SoxbJaq"
      },
      "source": [
        "## 1.2 The Network\n",
        "\n",
        "We will making a class called Network which has certain functions inside it. The cost function used is Cross-Entropy Loss. You need to code only the first 3. Rest are done for you.  There are various places within the code marked as stop_zone. Read the instructions below the code at those places to check whether your code till there is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwsydmyTEt0z"
      },
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self,sizes): # sizes is a list containing the network.\n",
        "                              # eg : [784,128,10] means input =784 neurons,\n",
        "                              #    1st hidden layer 128 neurons, output 10 neurons.\n",
        "        self.sizes=sizes\n",
        "        self.num_layers=\"...can you say the number of layers based on the list called sizes...\"\n",
        "        self.weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "        self.biases= \"...can you do this by understanding the self.weights...\"\n",
        "\n",
        "# stop_zone 1. Comment out all the code below. Select all rows below. Click Ctrl + /.\n",
        "# Include the show function given below above this comment area inside the class.\n",
        "# Run this cell and then run the code with stop_zone 1 written below.\n",
        "# After this testing, don't forget to remove the comments. Same, select all, Ctrl+/.\n",
        "\n",
        "    def forwardpropagation(self,a):\n",
        "        for b,w in zip(self.biases, self.weights):\n",
        "            a=\"...a is activation...\" # sig (w.a +b)\n",
        "            # print(a.shape)\n",
        "        return a\n",
        "\n",
        "# stop_zone 2. Comment out all the code below. Don't comment out the __init__ method else you will get error.\n",
        "# Remove comment from print(a.shape) line above. Run this cell. And run the code with stop_zone 2 written below.\n",
        "\n",
        "\n",
        "    def backpropagation(self,x,y):\n",
        "\n",
        "        # nothing to do in this 3 lines. it is for creating a one-hot encoded vector of the labels.\n",
        "        y_t = np.zeros((len(y), 10))\n",
        "        y_t[np.arange(len(y)), y] = 1\n",
        "        y_t= y_t.T\n",
        "\n",
        "        #nabla_b=dC/db and nabla_w=dC/dw. They are lists of shapes equal to that of bias and weights.\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # initially, a0 = input.\n",
        "        activation=x\n",
        "        activation_list=[x]\n",
        "\n",
        "        # step 1 : calculation of delta in last layer\n",
        "\n",
        "        # write the same forward propagation code here but while doing so store the a's.\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            activation= \"...just written above...\"\n",
        "            activation_list.append(activation)\n",
        "\n",
        "        delta= \"...delta is dC/dz3... how is it calculated?...\"\n",
        "\n",
        "        # step 2 : nabla_b and nabla_w relation with delta of last layer\n",
        "\n",
        "        nabla_b[-1]=\"...how is dC/db3 and dC/dz3 related...\"\n",
        "        nabla_w[-1]= \"...how is dC/dw3 and dC/dz3 related...\"\n",
        "\n",
        "        # print(\"{} {}\".format(nabla_b[-1].shape,nabla_w[-1].shape) )\n",
        "#stop_zone 3 : remove comment from the print statement just above and run the cell for stop_zone3.\n",
        "# don't forget commenting out.\n",
        "\n",
        "        # step 3 : calculation of delta for hidden layers\n",
        "\n",
        "        for j in range(2,self.num_layers):\n",
        "            sig_der = activation_list[-j]*(1-activation_list[-j])\n",
        "            delta= \"...how is dC/dz2 and dC/dz3 related ? Look i have calculated one term already for you (sig_der)...\"\n",
        "\n",
        "            # step 4 : nabla_b and nabla_w relation with delta of others layers\n",
        "            nabla_b[-j]= \"...again, how is dC/db2 and dC/dz2 related...\"\n",
        "            nabla_w[-j]= \"...how is dC/dw2 and dC/dz2 related...\"\n",
        "\n",
        "        return (nabla_b,nabla_w)\n",
        "#stop_zone 4 : Run the cell for stop_zone 4.\n",
        "\n",
        "    def SGD(self, train_data,epochs,mini_batch_size, lr):\n",
        "        n_train= len(train_data)\n",
        "        for i in range(epochs):\n",
        "            random.shuffle(train_data)\n",
        "            mini_batches = [\"\"\"...Split the data into batches each of size = mini_batch_size  ...\"\"\"]\n",
        "\n",
        "  # Stop zone 5 : Remove comment from the next print line and comment out all the lines below it.\n",
        "        # print(np.array(mini_batches, dtype=object).shape)\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch,lr)\n",
        "\n",
        "            self.predict(train_data)\n",
        "            print(\"Epoch {0} completed.\".format(i+1))\n",
        "\n",
        "    # the functions below are complete. If you are fine till stop_zone 5, you can run\n",
        "    # this whole cell and train, test the data by running the last cell of the notebook.\n",
        "    # You may need to wait for around 10 minutes to see the test predictions.\n",
        "\n",
        "    def update_mini_batch(self,mini_batch,lr):\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "        for x,y in mini_batch:\n",
        "            delta_b,delta_w= self.backpropagation(x,y)\n",
        "            nabla_b=[nb+ db for nb,db in zip (nabla_b,delta_b)]\n",
        "            nabla_w=[nw+dw for nw,dw in zip(nabla_w,delta_w)]\n",
        "\n",
        "        self.weights=[w- lr*nw/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
        "        self.biases=[b-lr*nb/len(mini_batch) for b,nb in zip(self.biases,nabla_b)]\n",
        "\n",
        "    def predict(self,test_data):\n",
        "        test_results = [(np.argmax(self.forwardpropagation(x)),y) for x,y in test_data]\n",
        "        # returns the index of that output neuron which has highest activation\n",
        "\n",
        "        num= sum(int (x==y) for x,y in test_results)\n",
        "        print (\"{0}/{1} classified correctly.\".format(num,len(test_data)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u8cVnGamVgP"
      },
      "outputs": [],
      "source": [
        "# stop_zone 1\n",
        "\n",
        "def show(self):\n",
        "  print(self.num_layers)\n",
        "  for bias in self.biases:\n",
        "      print(bias.shape)\n",
        "  for weight in self.weights:\n",
        "      print(weight.shape)\n",
        "\n",
        "# Copy this show function from here. Paste it inside that Network Class.\n",
        "# Comment out the show function here. Run this cell.\n",
        "\n",
        "net=Network([784,128,64,10])\n",
        "net.show()\n",
        "\n",
        "# The desired output is :\n",
        "# 4\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation.\n",
        "\n",
        "# Keeping the show function over there in the Network class doesn't make any\n",
        "# difference. You may delete it if you wish. Better toss a coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7EJBF7XsSft"
      },
      "outputs": [],
      "source": [
        "# stop_zone 2\n",
        "# to use this, make sure your data is loaded. Run this cell.\n",
        "net=Network([784,128,64,10])\n",
        "print(train_X[0])\n",
        "net.forwardpropagation(train_X[0])\n",
        "\n",
        "# The desired output is :\n",
        "# (784, 1)\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwHWyaKNhIIk"
      },
      "outputs": [],
      "source": [
        "# stop_zone 3\n",
        "net=Network([784,128,64,10])\n",
        "net.backpropagation(train_X[0],train_y[0])\n",
        "\n",
        "# Desired output : (10,1) (10,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pq4E3rHik-f"
      },
      "outputs": [],
      "source": [
        "# stop_zone 4\n",
        "net=Network([784,128,64,10])\n",
        "nabla_b,nabla_w=net.backpropagation(train_X[0],train_y[0])\n",
        "for nb in nabla_b:\n",
        "  print(nb.shape)\n",
        "for nw in nabla_w:\n",
        "  print(nw.shape)\n",
        "\n",
        "# Desired output:\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaCblO7XE0Fy"
      },
      "outputs": [],
      "source": [
        "# Stop zone 5 :  Run this cell, for 10000 samples and batch size of 20, output should be\n",
        "#       (500,20,2).  500 batches each of size 20 and has 2 objects : train and test data.\n",
        "\n",
        "net=Network([784,256,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=20,mini_batch_size=20,lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXljiAYRlvdq"
      },
      "outputs": [],
      "source": [
        "net=Network([784,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=10,mini_batch_size=20,lr=0.01)\n",
        "print(\"Test data:\")\n",
        "net.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhMIoFT9m7OU"
      },
      "source": [
        "# End of question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n"
      ],
      "metadata": {
        "id": "p50J5hpCdwYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('animation', html='html5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efq9Y2Jsd2ES",
        "outputId": "fc40b94f-498d-48f0-f829-fb690511cb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d57511d72f78>:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying the Songs\n",
        "\n",
        "We will be using the Spotify Songs and their Characteristics Dataset, You can access it from : {https://drive.google.com/file/d/178aXv-sSU13D5T4CxuzU3FfNDcz53OIT/view?usp=sharing}\n",
        "<br>\n",
        "Solve the Check Points one after another"
      ],
      "metadata": {
        "id": "rBtYGPnCfWJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "spotify = pd.read_csv('.... Add Input Data')\n",
        "\n",
        "X = spotify.copy().dropna()\n",
        "y = X.pop('track_popularity')\n",
        "artists = X['track_artist']\n",
        "\n",
        "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
        "                'speechiness', 'acousticness', 'instrumentalness',\n",
        "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
        "features_cat = ['playlist_genre']\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "  ### ######################################  Check Point 1     ########################\n",
        " # ...Encode the data\n",
        " # ... Make sure that you add unknown libraries\n",
        ")\n",
        "\n",
        "# We'll do a \"grouped\" split to keep all of an artist's songs in one\n",
        "# split or the other. This is to help prevent signal leakage.\n",
        "def group_split(X, y, group, train_size=0.75):\n",
        "    splitter = GroupShuffleSplit(train_size=train_size)\n",
        "    train, test = next(splitter.split(X, y, groups=group))\n",
        "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "y_train = y_train / 100 # popularity is on a scale 0-100, so this rescales to 0-1.\n",
        "y_valid = y_valid / 100\n",
        "\n",
        "input_shape = [X_train.shape[1]]\n",
        "print(\"Input shape: {}\".format(input_shape))"
      ],
      "metadata": {
        "id": "JOdK9iSkeS8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(1, input_shape=input_shape),\n",
        "])\n",
        "model.compile(\n",
        "    ######################## Check Point 2 ##########################################\n",
        "   ## Add loss adam, optimiser mae and parameters accuracy\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=50,\n",
        "    verbose=0, # suppress output since we'll plot the curves\n",
        ")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
        "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));\n",
        "import seaborn as sns\n",
        "sns.lineplot(data=history_df.loc[0:, ['accuracy', 'val_accuracy']])\n",
        "plt.title('Accuracy and Validation Accuracy')\n",
        "plt.show()\n",
        "print(\"Maximum Validation Accuracy: {:0.4f}\".format(history_df['val_accuracy'].max()))\n"
      ],
      "metadata": {
        "id": "xTaLTpisfhJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df.loc[10:, ['loss', 'val_loss']].plot()\n",
        "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
      ],
      "metadata": {
        "id": "77lfMGA8fwDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse the validation Loss and Check if it's underfitting or overfitting <br>\n",
        "Write 200 Words on what you think is happening ?"
      ],
      "metadata": {
        "id": "tNpTdyC1fvuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add some capacity to our network. We'll add three hidden layers with 128 units each. Run the next cell to train the network and see the learning curves."
      ],
      "metadata": {
        "id": "RMU1gCP1hd96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    ##################################### Check Point 3 #########################################\n",
        "    # Insert layers as suggested\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=50,\n",
        ")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
        "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
      ],
      "metadata": {
        "id": "FCiD0FdehDDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the Obtained Validation accuracy the best possible for the data. What is happening in the model and what is the reason for this behaviour explain in 200 Words."
      ],
      "metadata": {
        "id": "cIWyC3Dchx8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code to find best validation accuracy"
      ],
      "metadata": {
        "id": "1cFZJde5gPML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Code to stop the training if we have decreasing validation accuracy in 3 iterations\n",
        "# Write a function from Scratch, Don't use call backs directly\n",
        "############################################  Check point 4 #################################################"
      ],
      "metadata": {
        "id": "gCatdXQ8iITw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model with this"
      ],
      "metadata": {
        "id": "dXQT6v9NibgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}